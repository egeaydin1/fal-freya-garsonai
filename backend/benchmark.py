"""
GarsonAI Pipeline Benchmark
Tests different LLM + TTS combinations to find the fastest pipeline.

Usage:
  cd backend && source venv/bin/activate
  python benchmark.py

Tests:
  1. Gemini 2.5 Flash via fal OpenRouter
  2. GPT-4o-mini via fal OpenRouter  
  3. Cached phrase hit vs miss
  4. TTS streaming latency
  5. Full pipeline simulation (STT â†’ LLM â†’ TTS)
"""
import asyncio
import time
import os
import sys
import base64
import json

# Setup path
sys.path.insert(0, os.path.dirname(__file__))
os.environ.setdefault("DATABASE_URL", "sqlite:///./garsonai.db")

from core.config import get_settings
from openai import OpenAI

settings = get_settings()
os.environ["FAL_KEY"] = settings.FAL_KEY

import fal_client
from services.phrase_cache import load_or_generate_all, match_cached_phrase


def timer(label):
    """Context manager for timing blocks."""
    class Timer:
        def __init__(self):
            self.elapsed = 0
        def __enter__(self):
            self.start = time.time()
            return self
        def __exit__(self, *args):
            self.elapsed = (time.time() - self.start) * 1000
            print(f"  â±ï¸  {label}: {self.elapsed:.0f}ms")
    return Timer()


def test_llm(model: str, prompt: str) -> dict:
    """Test LLM response time (streaming TTFT + total)."""
    client = OpenAI(
        api_key="fal",
        base_url="https://fal.run/openrouter/router/openai/v1",
        default_headers={"Authorization": f"Key {settings.FAL_KEY}"}
    )

    system = """Sen GarsonAI. JSON formatÄ±nda yanÄ±t ver:
{"spoken_response":"yanÄ±t","intent":"recommend","product_name":"","product_id":null,"quantity":1,"recommendation":{"product_id":5,"product_name":"KÃ¼nefe","reason":"bugÃ¼nÃ¼n favorisi"}}"""

    menu = """ğŸ“‚ Ana Yemek:
  - ID:3 | Adana Kebap | 250â‚º
  - ID:5 | MantÄ± | 180â‚º
ğŸ“‚ TatlÄ±:
  - ID:14 | KÃ¼nefe | 150â‚º
  - ID:15 | Baklava | 140â‚º"""

    start = time.time()
    ttft = None
    full = ""

    stream = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system},
            {"role": "system", "content": f"MenÃ¼:\n{menu}"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.7,
        max_tokens=200,
        stream=True,
    )

    for chunk in stream:
        delta = chunk.choices[0].delta if chunk.choices else None
        if delta and delta.content:
            if ttft is None:
                ttft = (time.time() - start) * 1000
            full += delta.content

    total = (time.time() - start) * 1000

    return {
        "model": model,
        "ttft_ms": round(ttft or 0),
        "total_ms": round(total),
        "response_len": len(full),
        "response": full[:120],
    }


def test_tts(text: str) -> dict:
    """Test TTS streaming latency."""
    start = time.time()
    first_chunk_ms = None
    total_bytes = 0
    chunk_count = 0

    stream = fal_client.stream(
        "freya-mypsdi253hbk/freya-tts",
        arguments={"input": text, "voice": "zeynep", "speed": 1.0},
        path="/stream"
    )

    for event in stream:
        if "audio" in event:
            chunk_count += 1
            pcm = base64.b64decode(event["audio"])
            total_bytes += len(pcm)
            if first_chunk_ms is None:
                first_chunk_ms = (time.time() - start) * 1000
        if event.get("done"):
            break

    total = (time.time() - start) * 1000

    return {
        "text": text[:50],
        "first_chunk_ms": round(first_chunk_ms or 0),
        "total_ms": round(total),
        "chunks": chunk_count,
        "bytes": total_bytes,
        "audio_sec": round(total_bytes / (16000 * 2), 1),
    }


def test_phrase_cache() -> dict:
    """Test phrase cache hit latency."""
    # Load cache
    load_or_generate_all()

    results = {}
    test_phrases = [
        "Tabii ki, hemen Ã¶nerebileceÄŸim gÃ¼zel seÃ§enekler var. KÃ¼nefe bugÃ¼n Ã§ok gÃ¼zel olmuÅŸ.",
        "AnladÄ±m, bir bakayÄ±m sizin iÃ§in. MenÃ¼mÃ¼zde harika seÃ§enekler mevcut.",
        "Bu cÃ¼mle cache'de yok, normal TTS gerekecek.",
    ]

    for phrase in test_phrases:
        start = time.time()
        matched, audio, remaining = match_cached_phrase(phrase)
        elapsed = (time.time() - start) * 1000
        results[phrase[:40]] = {
            "hit": matched is not None,
            "lookup_ms": round(elapsed, 2),
            "audio_bytes": len(audio) if audio else 0,
            "remaining": remaining[:40] if remaining else "",
        }

    return results


def main():
    print("\n" + "="*70)
    print("ğŸï¸  GarsonAI Pipeline Benchmark")
    print("="*70)

    # â”€â”€ 1. LLM Tests â”€â”€
    print("\nğŸ“Š LLM Streaming Tests")
    print("-"*50)

    test_prompt = "Ne Ã¶nerirsin bana tatlÄ± istiyorum"
    models = [
        "google/gemini-2.5-flash",
        "openai/gpt-4o-mini",
    ]

    llm_results = []
    for model in models:
        print(f"\n  Testing {model}...")
        try:
            r = test_llm(model, test_prompt)
            llm_results.append(r)
            print(f"    TTFT: {r['ttft_ms']}ms | Total: {r['total_ms']}ms | Len: {r['response_len']}")
            print(f"    Response: {r['response'][:80]}...")
        except Exception as e:
            print(f"    âŒ Error: {e}")

    # â”€â”€ 2. TTS Tests â”€â”€
    print("\nğŸ“Š TTS Streaming Tests")
    print("-"*50)

    tts_tests = [
        "Tabii ki, hemen Ã¶nerebileceÄŸim gÃ¼zel seÃ§enekler var.",
        "Peki, hemen halledelim!",
        "KÃ¼nefe bugÃ¼nÃ¼n en Ã§ok tercih edilen tatlÄ±sÄ±, kesinlikle denemelisiniz! 150 lira.",
    ]

    tts_results = []
    for text in tts_tests:
        print(f"\n  Testing: '{text[:40]}...'")
        try:
            r = test_tts(text)
            tts_results.append(r)
            print(f"    First chunk: {r['first_chunk_ms']}ms | Total: {r['total_ms']}ms | Audio: {r['audio_sec']}s")
        except Exception as e:
            print(f"    âŒ Error: {e}")

    # â”€â”€ 3. Phrase Cache Tests â”€â”€
    print("\nğŸ“Š Phrase Cache Tests")
    print("-"*50)

    cache_results = test_phrase_cache()
    for phrase, r in cache_results.items():
        hit = "âœ… HIT" if r["hit"] else "âŒ MISS"
        print(f"  {hit} '{phrase}...' â†’ {r['lookup_ms']}ms, {r['audio_bytes']} bytes")

    # â”€â”€ 4. Summary â”€â”€
    print("\n" + "="*70)
    print("ğŸ“Š SUMMARY - Latency Targets")
    print("="*70)

    if llm_results:
        best_llm = min(llm_results, key=lambda x: x["ttft_ms"])
        print(f"\n  ğŸ† Fastest LLM TTFT: {best_llm['model']} â†’ {best_llm['ttft_ms']}ms")

    if tts_results:
        best_tts = min(tts_results, key=lambda x: x["first_chunk_ms"])
        print(f"  ğŸ† Fastest TTS first chunk: {best_tts['first_chunk_ms']}ms for '{best_tts['text']}'")

    print(f"\n  ğŸ¯ With cached phrase: ~0ms first audio (instant playback)")
    print(f"  ğŸ¯ Without cache: LLM TTFT + TTS first chunk")
    if llm_results and tts_results:
        worst = max(r["ttft_ms"] for r in llm_results) + max(r["first_chunk_ms"] for r in tts_results)
        best = min(r["ttft_ms"] for r in llm_results) + min(r["first_chunk_ms"] for r in tts_results)
        print(f"  ğŸ¯ Best case (no cache): ~{best:.0f}ms")
        print(f"  ğŸ¯ Worst case (no cache): ~{worst:.0f}ms")

    print("\n" + "="*70)
    print("  Recommendation: Use Gemini 2.5 Flash + phrase cache for <200ms first audio")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
