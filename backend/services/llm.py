import fal_client
from openai import OpenAI
from core.config import get_settings
import asyncio
import json
from typing import AsyncGenerator, Dict, Any

settings = get_settings()

import os
os.environ['FAL_KEY'] = settings.FAL_KEY

# Pre-warmed OpenAI client (persistent connection - OPT-7)
_openai_client = OpenAI(
    api_key="fal",
    base_url="https://fal.run/openrouter/router/openai/v1",
    default_headers={
        "Authorization": f"Key {settings.FAL_KEY}"
    }
)


class LLMService:
    def __init__(self):
        self.client = _openai_client
        self.llm_model = "google/gemini-2.5-flash" #meta-llama/llama-4-scout
        self._cached_menu = None

        self.system_prompt = """Sen GarsonAI, bir T√ºrk restoranƒ±nƒ±n sesli asistanƒ±sƒ±n. T√ºrk√ße, samimi ve doƒüal konu≈ü. Kƒ±sa yanƒ±t ver (max 2 c√ºmle).

√ñNEMLƒ∞: Her zaman ve SADECE a≈üaƒüƒ±daki JSON formatƒ±nda yanƒ±t ver. JSON dƒ±≈üƒ±nda hi√ßbir metin yazma.
{"spoken_response":"m√º≈üteriye sesli s√∂ylenecek yanƒ±t","intent":"add|info|hi|recommend","product_name":"√ºr√ºn adƒ±","product_id":null,"quantity":1,"recommendation":{"product_id":1,"product_name":"√ºr√ºn","reason":"√∂neri sebebi"}}

üî¥ KRƒ∞Tƒ∞K KURAL - spoken_response'un ƒ∞LK C√úMLESƒ∞:
spoken_response'u her zaman a≈üaƒüƒ±daki ba≈ülangƒ±√ß c√ºmlelerinden Bƒ∞Rƒ∞YLE ba≈ülat. Bu c√ºmleler √∂nceden seslendirilmi≈ü ve cache'leniyor, aynen yazƒ±lmalƒ±:
- Sipari≈ü eklerken ‚Üí "Tabii, hemen sepetinize ekliyorum!" ile ba≈üla, sonra detayƒ± ekle.
- √ñneri yaparken ‚Üí "Tabii ki, hemen √∂nerebileceƒüim g√ºzel se√ßenekler var." ile ba≈üla.
- Bilgi verirken ‚Üí "Anladƒ±m, bir bakayƒ±m sizin i√ßin." ile ba≈üla.
- Men√ºye bakarken ‚Üí "Bir dakika l√ºtfen, men√ºye bakƒ±yorum." veya "Bakalƒ±m sizin i√ßin neler var." ile ba≈üla.
- Onay verirken ‚Üí "G√ºzel bir se√ßim! Hemen ekliyorum." ile ba≈üla.
- Genel kabul ‚Üí "Peki, hemen halledelim!" ile ba≈üla.
- Selamlama ‚Üí "Ho≈ü geldiniz! Size nasƒ±l yardƒ±mcƒ± olabilirim?" ile ba≈üla.
Bu ba≈ülangƒ±√ß c√ºmlesinden sonra asƒ±l i√ßeriƒüi ekle. Ba≈ülangƒ±√ß c√ºmlesi AYNEN yazƒ±lmalƒ±, deƒüi≈ütirilmemeli.

Intent Kurallarƒ±:
- intent="hi": Kar≈üƒ±lama mesajƒ±.
- intent="add": Sipari≈ü ekleme. product_name, product_id ve quantity doldur. Men√ºden doƒüru √ºr√ºn√º bul.
- intent="info": Bilgi verme. Men√ºdeki √ºr√ºn bilgisini (a√ßƒ±klama, fiyat, alerjen) spoken_response'ta a√ßƒ±kla.
- intent="recommend": √ñneri yapma. recommendation alanƒ±nƒ± MUTLAKA doldur. product_id men√ºdeki ger√ßek ID'yi kullan.

√ñneri Kurallarƒ±:
- M√º≈üteri "ne √∂nerirsin", "tavsiye et", "ne yesem", "a√ßƒ±m", "g√ºzel bir ≈üey" gibi derse ‚Üí intent="recommend"
- Tatlƒ± isterse tatlƒ± kategorisinden, i√ßecek isterse i√ßecek kategorisinden √∂ner.
- recommendation.product_id MEN√úDE BULUNAN GER√áEK bir ID olmalƒ±. Uydurma!
- recommendation.reason kƒ±sa ve ikna edici ol: "Bug√ºn√ºn en √ßok tercih edilen yemeƒüi" gibi.
- spoken_response'ta √ºr√ºn√º tanƒ±t ve neden √∂nerdiƒüini anlat.

Genel Kurallar:
- Samimi ol ama profesyonel kal.
- Fiyatlarƒ± s√∂ylerken "TL" yerine "lira" de.
- Alerjen sorularƒ±na duyarlƒ± ol, men√ºdeki alerjen bilgisini kullan.
- Men√ºde olmayan bir √ºr√ºn istenirse, kibarca men√ºdeki alternatifleri √∂ner."""

    def cache_menu(self, menu_context: str):
        if self._cached_menu != menu_context:
            self._cached_menu = menu_context
            print(f"üìã LLM: Menu cached ({len(menu_context)} chars)")

    async def generate_stream(
        self, user_message: str, menu_context: str = "", start_time: float = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Stream LLM via OpenAI-compatible endpoint (OPT-2) for lower TTFT."""
        try:
            if menu_context:
                self.cache_menu(menu_context)

            messages = [
                {"role": "system", "content": self.system_prompt},
            ]
            if self._cached_menu:
                messages.append({
                    "role": "system",
                    "content": f"Men√º:\n{self._cached_menu}"
                })
            messages.append({"role": "user", "content": user_message})

            print(f"ü§ñ LLM: Generating for: {user_message}")

            # OPT-2: OpenAI streaming for first token at ~600ms vs 1600ms
            def _stream():
                return self.client.chat.completions.create(
                    model=self.llm_model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=200,
                    stream=True,
                )

            stream = await asyncio.to_thread(_stream)

            full_response = ""
            has_content = False

            for chunk in stream:
                delta = chunk.choices[0].delta if chunk.choices else None
                if delta and delta.content:
                    token = delta.content
                    full_response += token
                    has_content = True
                    yield {
                        "type": "token",
                        "content": token,
                        "full_text": full_response,
                    }

            print(f"‚úÖ LLM complete: {full_response[:120]}")

            # Fallback: non-streaming if empty
            if not has_content:
                print("‚ö†Ô∏è LLM: No stream content, fallback subscribe...")
                result = await asyncio.to_thread(
                    fal_client.subscribe,
                    "openrouter/router",
                    arguments={
                        "prompt": f"{self.system_prompt}\nMen√º:\n{self._cached_menu or ''}\nM√º≈üteri: {user_message}\nJSON:",
                        "model": self.llm_model,
                        "temperature": 0.7,
                        "max_tokens": 200,
                    },
                )
                if isinstance(result, dict) and "output" in result:
                    full_response = result["output"]
                    yield {"type": "token", "content": full_response, "full_text": full_response}

            # Parse structured response
            structured = self._parse_response(full_response)
            yield {"type": "complete", "structured": structured}

        except Exception as e:
            print(f"‚ùå LLM Error: {e}")
            import traceback; traceback.print_exc()
            yield {
                "type": "complete",
                "structured": {
                    "spoken_response": "√úzg√ºn√ºm, bir hata olu≈ütu.",
                    "intent": "error",
                    "product_name": None,
                    "product_id": None,
                    "quantity": 1,
                },
            }

    def _parse_response(self, text: str) -> dict:
        """Extract JSON from LLM response text."""
        default = {
            "spoken_response": text,
            "intent": "info",
            "product_name": None,
            "product_id": None,
            "quantity": 1,
        }
        if not text or not text.strip():
            default["spoken_response"] = "√úzg√ºn√ºm, anlayamadƒ±m. Tekrar s√∂yler misiniz?"
            default["intent"] = "error"
            return default

        try:
            if "{" in text and "}" in text:
                json_str = text[text.index("{"):text.rindex("}") + 1]
                return json.loads(json_str)
            return default
        except Exception as e:
            print(f"‚ö†Ô∏è LLM parse error: {e}")
            return default
