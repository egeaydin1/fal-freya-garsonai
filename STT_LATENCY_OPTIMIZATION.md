# ğŸš€ STT Latency Optimization Report

**Tarih:** 12 Åubat 2026  
**Hedef:** Whisper inference sÃ¼resini deÄŸiÅŸtirmeden toplam STT latency'sini azaltmak  
**SonuÃ§:** 9-10 saniye â†’ **4-6 saniye** (beklenen)

---

## ğŸ“Š Problem Analizi

Ã–nceki loglardan:

```
[STT done]: 09.6s
```

Bu sÃ¼re **sadece inference deÄŸil**, ÅŸunlarÄ±n toplamÄ±:

| BileÅŸen | Tahmini SÃ¼re | Kontrol Edilebilir? |
|---------|--------------|---------------------|
| **Upload (CDN)** | 1.5-3s | âœ… Evet |
| **Queue wait** | 0.5-2s | âœ… KÄ±smen (warmer ile) |
| **Cold start** | 2-3s | âœ… Evet (warmer) |
| **Inference** | 1-2s | âŒ HayÄ±r (model sabit) |
| **Network RTT** | 0.1-0.5s | âœ… KÄ±smen (region) |

**Toplam:** ~6-11 saniye

**Kritik Fark:** Inference sÃ¼resi sadece 1-2 saniye! Geri kalan 7-8 saniye orchestration overhead.

---

## âœ… Uygulanan Optimizasyonlar

### 1. ğŸ¯ Direct Binary POST (CDN Bypass)

**Ã–nceki AkÄ±ÅŸ:**
```
1. audio_bytes â†’ temp file yaz
2. fal_client.upload_file() â†’ CDN'e upload
3. CDN URL dÃ¶ner
4. fal_client.subscribe(audio_url=cdn_url)
5. Container CDN'den dosyayÄ± indirir
6. Inference baÅŸlar
```

**Toplam:** ~3-5 saniye overhead

**Yeni AkÄ±ÅŸ:**
```
1. audio_bytes â†’ multipart/form-data
2. httpx.post(files={"audio": bytes})
3. Inference hemen baÅŸlar
```

**KazanÃ§:** 1.5-3 saniye âš¡

**Kod:**
```python
# backend/services/stt.py
files = {
    "audio": ("audio.webm", io.BytesIO(audio_data), "audio/webm")
}

response = await self.http_client.post(
    self.api_url,
    files=files,
    data={
        "task": "transcribe",
        "language": "tr",
        "chunk_level": "segment"
    },
    headers={"Authorization": f"Key {settings.FAL_KEY}"}
)
```

**Not:** EÄŸer fal API multipart desteklemiyorsa, fallback olarak eski yÃ¶ntem kullanÄ±lÄ±r.

---

### 2. âœ‚ï¸ Silence Trimming

**Sorun:** KullanÄ±cÄ± konuÅŸtuktan sonra:
- BaÅŸta 200ms sessizlik
- Sonda 1-2 saniye sessizlik

Whisper bu sessizlikleri de iÅŸliyor â†’ gereksiz inference sÃ¼resi.

**Ã‡Ã¶zÃ¼m:** Frontend'te RMS analizi ile sessizlikleri kes.

**Kod:**
```javascript
// frontend/src/utils/AudioTrimmer.js
const startIndex = this._findFirstNonSilence(channelData, sampleRate);
const endIndex = this._findLastNonSilence(channelData, sampleRate);
```

**Ã–rnek:**
- Orijinal: 4 saniye audio (1s sessizlik + 2s konuÅŸma + 1s sessizlik)
- Trimmed: 2.2 saniye audio (0.1s padding + 2s konuÅŸma + 0.1s padding)

**KazanÃ§:** %30-40 daha hÄ±zlÄ± inference (~0.6-1.2 saniye)

**Entegrasyon:**
```javascript
// VoiceAI.jsx
const trimmedBlob = await trimmerRef.current.trimSilence(fullAudioBlob);
const compressedBlob = await compressorRef.current.compressAudio(trimmedBlob);
wsRef.current.send(compressedBlob);
```

---

### 3. ğŸ”¥ Aggressive Container Warming

**Ã–nceki:**
- TTS warmer: 30 saniye interval
- STT warmer: yok

**Sorun:** fal container idle timeout ~60-120 saniye
- 30s interval â†’ bazÄ± istekler cold start yaÅŸÄ±yor
- STT her zaman cold start

**Yeni:**
```python
# backend/services/tts_warmer.py
def __init__(self, interval: int = 20):  # 30s â†’ 20s

async def run(self):
    await asyncio.gather(
        self.warmup_tts(),    # TTS container
        self.warmup_stt(),    # STT container (YENÄ°!)
        return_exceptions=True
    )
```

**KazanÃ§:** Cold start 2-3s â†’ 0-0.5s (~2 saniye) âš¡

**Maliyet:** Her 20 saniyede 2 test API Ã§aÄŸrÄ±sÄ± (minimal)

---

### 4. âš¡ Parallel Processing (Zaten Mevcut)

WebSocket handler'da zaten paralel TTS var:

```python
# Ä°lk cÃ¼mle tamamlanÄ±nca TTS baÅŸlatÄ±lÄ±yor
if first_sentence_complete:
    tts_task = asyncio.create_task(stream_tts_parallel())
```

Bu sayede LLM devam ederken TTS baÅŸlÄ±yor.

**Not:** STT iÃ§in parallelization ÅŸu an gerekli deÄŸil Ã§Ã¼nkÃ¼ STT tek seferde yapÄ±lÄ±yor.

---

### 5. ğŸ“Š Enhanced Logging

**Yeni loglar:**

```python
# DetaylÄ± timing breakdown
print(f"ğŸ“¡ STT: HTTP request took {t_response - t_request:.3f}s")
print(f"âœ… [STT done]: {elapsed:06.3f}s total | {request_time:.3f}s request")
print(f"âœ… [STT done]: {elapsed:06.3f}s total | upload: {upload_time:.3f}s | inference: {inference_time:.3f}s")
```

Bu sayede hangi aÅŸamada problem olduÄŸu net gÃ¶rÃ¼nÃ¼yor.

---

## ğŸ“ˆ Beklenen SonuÃ§lar

### Ã–nceki Timing:
```
[00:00.000] Audio received
[00:03.500] Upload complete
[00:05.500] Queue start (cold container)
[00:09.600] STT complete âŒ
```

### Yeni Timing (Best Case):
```
[00:00.000] Audio received
[00:00.100] Trimming complete (-40% audio)
[00:00.200] Direct POST start
[00:00.500] Inference start (warm container)
[00:01.500] Inference complete (1s audio)
[00:01.600] STT complete âœ…
```

### Yeni Timing (Realistic):
```
[00:00.000] Audio received
[00:00.150] Trimming complete (-30% audio)
[00:00.250] Direct POST start
[00:01.000] Inference start (warm/cold mix)
[00:02.500] Inference complete (1.5s audio)
[00:02.700] STT complete âœ…
```

**Toplam:** 4-6 saniye (Ã¶nceki: 9-10 saniye)

---

## ğŸ¯ Region KontrolÃ¼ (Manuel)

fal.ai dashboard'dan kontrol et:

1. https://fal.ai/dashboard â†’ Settings
2. Default region nedir?
   - US-East (Virginia) âœ… En hÄ±zlÄ±
   - EU-West (Frankfurt) âš ï¸ +50-100ms
   - AP-Southeast (Singapore) âŒ +200-300ms

3. EÄŸer EU region kullanÄ±lÄ±yorsa:
   - Latency: +100-200ms
   - DeÄŸiÅŸtirilebilir mi? â†’ fal API docs kontrol et

---

## ğŸ”¬ Test SenaryolarÄ±

### Test 1: Direct POST vs CDN Upload
```bash
# Backend logs'u izle
tail -f backend.log

# Frontend'ten ses kaydÄ± yap (3 saniye konuÅŸma)
# Loglarda ara:
âœ… "Using direct binary POST (CDN bypass)" â†’ Direct POST Ã§alÄ±ÅŸÄ±yor
âš ï¸ "Direct POST failed, falling back..." â†’ Fallback'e dÃ¼ÅŸtÃ¼
```

**Beklenen:** Direct POST baÅŸarÄ±lÄ± olmalÄ±

### Test 2: Silence Trimming Etkisi
```javascript
// Browser console'da kontrol et
ğŸ“‰ Original audio: 35000 bytes
âœ‚ï¸ AudioTrimmer: Trimmed 35.2% (3.50s â†’ 2.27s)
âœ… Final audio: 22000 bytes
```

**Beklenen:** %20-40 reduction

### Test 3: Container Warm/Cold
```bash
# Ä°lk istek (container cold)
[STT done]: 05.200s total | 3.800s request

# 15 saniye sonra ikinci istek (container warm)
[STT done]: 02.100s total | 0.900s request âœ…
```

**Beklenen:** 2. istek Ã§ok daha hÄ±zlÄ±

---

## âš ï¸ Bilinen SÄ±nÄ±rlamalar

### 1. fal API Multipart DesteÄŸi Belirsiz
EÄŸer fal API direct multipart desteklemiyorsa:
- Fallback: CDN upload (eski yÃ¶ntem)
- Test gerekli

**Alternatif:**
```python
# Base64 encoding (daha yavaÅŸ ama CDN'siz)
audio_b64 = base64.b64encode(audio_data).decode('utf-8')
```

### 2. AudioTrimmer Performance
Browser'da AudioContext decoding:
- 3 saniyelik audio: ~50-100ms overhead
- 10 saniyelik audio: ~200-300ms overhead

**Ã‡Ã¶zÃ¼m:** Acceptable trade-off (inference kazancÄ± > decoding overhead)

### 3. Warmer Maliyeti
Her 20 saniyede 2 API call:
- GÃ¼nlÃ¼k: 2 * 3 * 60 * 24 = 8,640 call
- AylÄ±k: ~260,000 call

**Optimizasyon:**
- Sadece aktif saatlerde Ã§alÄ±ÅŸtÄ±r
- Veya interval'i 30s'e geri Ã§ek

---

## ğŸš€ Gelecek Optimizasyonlar

### 1. Segment-based Pseudo Streaming
KullanÄ±cÄ± konuÅŸurken her 1 saniyede bir segment gÃ¶nder:

```javascript
// Her 1 saniyede bir STT Ã§aÄŸrÄ±sÄ±
setInterval(() => {
    if (isRecording && audioChunks.length > 0) {
        sendPartialAudio();  // Partial transcription
    }
}, 1000);
```

**KazanÃ§:** KullanÄ±cÄ± konuÅŸurken STT baÅŸlar (~2-3s)

**Zorluk:** fal API partial result destekliyor mu?

### 2. Edge Computing
CDN yerine Cloudflare Workers'da STT:
- RTT: <50ms
- Ama Whisper Ã§alÄ±ÅŸtÄ±ramaz

**Alternatif:** WebAssembly Whisper (browser'da)
- https://github.com/ggerganov/whisper.cpp
- WASM build â†’ browser'da inference
- Latency: <1s âš¡âš¡âš¡

### 3. Custom Model Deployment
Kendi Whisper container'Ä±:
- fal.ai yerine kendi sunucusu
- Warm container 7/24
- Region control

**Maliyet:** ~$50-100/ay

---

## ğŸ“ Ã–zet

| Optimizasyon | KazanÃ§ | Zorluk | Durum |
|-------------|--------|--------|-------|
| Direct Binary POST | 1.5-3s | Orta | âœ… UygulandÄ± |
| Silence Trimming | 0.6-1.2s | DÃ¼ÅŸÃ¼k | âœ… UygulandÄ± |
| Aggressive Warmer | ~2s | DÃ¼ÅŸÃ¼k | âœ… UygulandÄ± |
| Region Optimization | 0.1-0.2s | DÃ¼ÅŸÃ¼k | âš ï¸ Manuel kontrol |
| Enhanced Logging | - | DÃ¼ÅŸÃ¼k | âœ… UygulandÄ± |

**Toplam Beklenen KazanÃ§:** 4-6 saniye

**Ã–nceki:** 9-10 saniye  
**SonrasÄ±:** 4-6 saniye âš¡

---

## ğŸ§ª Test Checklist

- [ ] Backend'i yeniden baÅŸlat
- [ ] Frontend build et
- [ ] Ä°lk test: Cold start timing
- [ ] Ä°kinci test (20s sonra): Warm container timing
- [ ] Browser console'da trimming loglarÄ±nÄ± kontrol et
- [ ] Backend'de "Direct POST" veya "fallback" mesajÄ±nÄ± kontrol et
- [ ] Timing breakdown'larÄ± karÅŸÄ±laÅŸtÄ±r

---

## ğŸ“ Troubleshooting

### "Direct POST failed" MesajÄ±
**Sebep:** fal API multipart desteklemiyor  
**Ã‡Ã¶zÃ¼m:** Fallback CDN upload kullanÄ±lacak (yine de warmer + trimming kazancÄ± var)

### Trimming %10'dan Az
**Sebep:** KullanÄ±cÄ± hemen konuÅŸup hemen bitiyor  
**Ã‡Ã¶zÃ¼m:** Normal, sessizlik zaten az

### Warmer Ã‡alÄ±ÅŸmÄ±yor
**Sebep:** Backend restart gerekli  
**Ã‡Ã¶zÃ¼m:**
```bash
cd backend
python main.py
# Logs'ta ÅŸunu ara:
# âœ… Container Warmer: Background task started (TTS + STT)
```

---

**Son GÃ¼ncelleme:** 12 Åubat 2026  
**Yazan:** AI Assistant  
**Versiyon:** 2.0
